{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgDBvgEeZzbuutSpxIGjKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aj1365/e-TransUNet/blob/main/eTransUNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljWGLL3M0cal"
      },
      "outputs": [],
      "source": [
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "import keras\n",
        "import keras.callbacks\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import backend as keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tfk.layers\n",
        "tfm = tf.math\n",
        "L2_WEIGHT_DECAY = 1e-4"
      ],
      "metadata": {
        "id": "Rw52zOGC0kwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics to be used when evaluating the network\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "precision = tf.keras.metrics.Precision()\n",
        "recall = tf.keras.metrics.Recall()\n",
        "f1 = F1Score(num_classes=2, name='f1', average='micro', threshold=0.4)\n",
        "sgd_optimizer = Adam()"
      ],
      "metadata": {
        "id": "gvk5_7jC0s8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ViT_MLP(X, filter_num, activation='GELU', name='MLP'):\n",
        "\n",
        "    activation_func = eval(activation)\n",
        "\n",
        "    for i, f in enumerate(filter_num):\n",
        "        X = Dense(f, name='{}_dense_{}'.format(name, i))(X)\n",
        "        X = activation_func(name='{}_activation_{}'.format(name, i))(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def ViT_block(V, num_heads, key_dim, filter_num_MLP, activation='GELU', name='ViT'):\n",
        "\n",
        "    # Multiheaded self-attention (MSA)\n",
        "    V_atten = V # <--- skip\n",
        "    V_atten = LayerNormalization(name='{}_layer_norm_1'.format(name))(V_atten)\n",
        "    V_atten = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim,\n",
        "                                 name='{}_atten'.format(name))(V_atten, V_atten)\n",
        "    # Skip connection\n",
        "    V_add = add([V_atten, V], name='{}_skip_1'.format(name)) # <--- skip\n",
        "\n",
        "    # MLP\n",
        "    V_MLP = V_add # <--- skip\n",
        "    V_MLP = LayerNormalization(name='{}_layer_norm_2'.format(name))(V_MLP)\n",
        "    V_MLP = ViT_MLP(V_MLP, filter_num_MLP, activation, name='{}_mlp'.format(name))\n",
        "    # Skip connection\n",
        "    V_out = add([V_MLP, V_add, V], name='{}_skip_2'.format(name)) # <--- skip\n",
        "\n",
        "    return V_out"
      ],
      "metadata": {
        "id": "4nMe477F0xDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def spatial_transformer_network(input_fmap, theta, out_dims=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Spatial Transformer Network layer implementation as described in [1].\n",
        "    The layer is composed of 3 elements:\n",
        "    - localization_net: takes the original image as input and outputs\n",
        "      the parameters of the affine transformation that should be applied\n",
        "      to the input image.\n",
        "    - affine_grid_generator: generates a grid of (x,y) coordinates that\n",
        "      correspond to a set of points where the input should be sampled\n",
        "      to produce the transformed output.\n",
        "    - bilinear_sampler: takes as input the original image and the grid\n",
        "      and produces the output image using bilinear interpolation.\n",
        "    Input\n",
        "    -----\n",
        "    - input_fmap: output of the previous layer. Can be input if spatial\n",
        "      transformer layer is at the beginning of architecture. Should be\n",
        "      a tensor of shape (B, H, W, C).\n",
        "    - theta: affine transform tensor of shape (B, 6). Permits cropping,\n",
        "      translation and isotropic scaling. Initialize to identity matrix.\n",
        "      It is the output of the localization network.\n",
        "    Returns\n",
        "    -------\n",
        "    - out_fmap: transformed input feature map. Tensor of size (B, H, W, C).\n",
        "    Notes\n",
        "    -----\n",
        "    [1]: 'Spatial Transformer Networks', Jaderberg et. al,\n",
        "         (https://arxiv.org/abs/1506.02025)\n",
        "    \"\"\"\n",
        "    # grab input dimensions\n",
        "    B = tf.shape(input_fmap)[0]\n",
        "    H = tf.shape(input_fmap)[1]\n",
        "    W = tf.shape(input_fmap)[2]\n",
        "\n",
        "    # reshape theta to (B, 2, 3)\n",
        "    theta = tf.reshape(theta, [B, 2, 3])\n",
        "\n",
        "    # generate grids of same size or upsample/downsample if specified\n",
        "    if out_dims:\n",
        "        out_H = out_dims[0]\n",
        "        out_W = out_dims[1]\n",
        "        batch_grids = affine_grid_generator(out_H, out_W, theta)\n",
        "    else:\n",
        "        batch_grids = affine_grid_generator(H, W, theta)\n",
        "\n",
        "    x_s = batch_grids[:, 0, :, :]\n",
        "    y_s = batch_grids[:, 1, :, :]\n",
        "\n",
        "    # sample input with grid to get output\n",
        "    out_fmap = bilinear_sampler(input_fmap, x_s, y_s)\n",
        "\n",
        "    return out_fmap\n",
        "\n",
        "\n",
        "def get_pixel_value(img, x, y):\n",
        "    \"\"\"\n",
        "    Utility function to get pixel value for coordinate\n",
        "    vectors x and y from a  4D tensor image.\n",
        "    Input\n",
        "    -----\n",
        "    - img: tensor of shape (B, H, W, C)\n",
        "    - x: flattened tensor of shape (B*H*W,)\n",
        "    - y: flattened tensor of shape (B*H*W,)\n",
        "    Returns\n",
        "    -------\n",
        "    - output: tensor of shape (B, H, W, C)\n",
        "    \"\"\"\n",
        "    shape = tf.shape(x)\n",
        "    batch_size = shape[0]\n",
        "    height = shape[1]\n",
        "    width = shape[2]\n",
        "\n",
        "    batch_idx = tf.range(0, batch_size)\n",
        "    batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
        "    b = tf.tile(batch_idx, (1, height, width))\n",
        "\n",
        "    indices = tf.stack([b, y, x], 3)\n",
        "\n",
        "    return tf.gather_nd(img, indices)\n",
        "\n",
        "\n",
        "def affine_grid_generator(height, width, theta):\n",
        "    \"\"\"\n",
        "    This function returns a sampling grid, which when\n",
        "    used with the bilinear sampler on the input feature\n",
        "    map, will create an output feature map that is an\n",
        "    affine transformation [1] of the input feature map.\n",
        "    Input\n",
        "    -----\n",
        "    - height: desired height of grid/output. Used\n",
        "      to downsample or upsample.\n",
        "    - width: desired width of grid/output. Used\n",
        "      to downsample or upsample.\n",
        "    - theta: affine transform matrices of shape (num_batch, 2, 3).\n",
        "      For each image in the batch, we have 6 theta parameters of\n",
        "      the form (2x3) that define the affine transformation T.\n",
        "    Returns\n",
        "    -------\n",
        "    - normalized grid (-1, 1) of shape (num_batch, 2, H, W).\n",
        "      The 2nd dimension has 2 components: (x, y) which are the\n",
        "      sampling points of the original image for each point in the\n",
        "      target image.\n",
        "    Note\n",
        "    ----\n",
        "    [1]: the affine transformation allows cropping, translation,\n",
        "         and isotropic scaling.\n",
        "    \"\"\"\n",
        "    num_batch = tf.shape(theta)[0]\n",
        "\n",
        "    # create normalized 2D grid\n",
        "    x = tf.linspace(-1.0, 1.0, width)\n",
        "    y = tf.linspace(-1.0, 1.0, height)\n",
        "    x_t, y_t = tf.meshgrid(x, y)\n",
        "\n",
        "    # flatten\n",
        "    x_t_flat = tf.reshape(x_t, [-1])\n",
        "    y_t_flat = tf.reshape(y_t, [-1])\n",
        "\n",
        "    # reshape to [x_t, y_t , 1] - (homogeneous form)\n",
        "    ones = tf.ones_like(x_t_flat)\n",
        "    sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])\n",
        "\n",
        "    # repeat grid num_batch times\n",
        "    sampling_grid = tf.expand_dims(sampling_grid, axis=0)\n",
        "    sampling_grid = tf.tile(sampling_grid, tf.stack([num_batch, 1, 1]))\n",
        "\n",
        "    # cast to float32 (required for matmul)\n",
        "    theta = tf.cast(theta, 'float32')\n",
        "    sampling_grid = tf.cast(sampling_grid, 'float32')\n",
        "\n",
        "    # transform the sampling grid - batch multiply\n",
        "    batch_grids = tf.matmul(theta, sampling_grid)\n",
        "    # batch grid has shape (num_batch, 2, H*W)\n",
        "\n",
        "    # reshape to (num_batch, H, W, 2)\n",
        "    batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n",
        "\n",
        "    return batch_grids\n",
        "\n",
        "\n",
        "def bilinear_sampler(img, x, y):\n",
        "    \"\"\"\n",
        "    Performs bilinear sampling of the input images according to the\n",
        "    normalized coordinates provided by the sampling grid. Note that\n",
        "    the sampling is done identically for each channel of the input.\n",
        "    To test if the function works properly, output image should be\n",
        "    identical to input image when theta is initialized to identity\n",
        "    transform.\n",
        "    Input\n",
        "    -----\n",
        "    - img: batch of images in (B, H, W, C) layout.\n",
        "    - grid: x, y which is the output of affine_grid_generator.\n",
        "    Returns\n",
        "    -------\n",
        "    - out: interpolated images according to grids. Same size as grid.\n",
        "    \"\"\"\n",
        "    H = tf.shape(img)[1]\n",
        "    W = tf.shape(img)[2]\n",
        "    max_y = tf.cast(H - 1, 'int32')\n",
        "    max_x = tf.cast(W - 1, 'int32')\n",
        "    zero = tf.zeros([], dtype='int32')\n",
        "\n",
        "    # rescale x and y to [0, W-1/H-1]\n",
        "    x = tf.cast(x, 'float32')\n",
        "    y = tf.cast(y, 'float32')\n",
        "    x = 0.5 * ((x + 1.0) * tf.cast(max_x-1, 'float32'))\n",
        "    y = 0.5 * ((y + 1.0) * tf.cast(max_y-1, 'float32'))\n",
        "\n",
        "    # grab 4 nearest corner points for each (x_i, y_i)\n",
        "    x0 = tf.cast(tf.floor(x), 'int32')\n",
        "    x1 = x0 + 1\n",
        "    y0 = tf.cast(tf.floor(y), 'int32')\n",
        "    y1 = y0 + 1\n",
        "\n",
        "    # clip to range [0, H-1/W-1] to not violate img boundaries\n",
        "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
        "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
        "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
        "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
        "\n",
        "    # get pixel value at corner coords\n",
        "    Ia = get_pixel_value(img, x0, y0)\n",
        "    Ib = get_pixel_value(img, x0, y1)\n",
        "    Ic = get_pixel_value(img, x1, y0)\n",
        "    Id = get_pixel_value(img, x1, y1)\n",
        "\n",
        "    # recast as float for delta calculation\n",
        "    x0 = tf.cast(x0, 'float32')\n",
        "    x1 = tf.cast(x1, 'float32')\n",
        "    y0 = tf.cast(y0, 'float32')\n",
        "    y1 = tf.cast(y1, 'float32')\n",
        "\n",
        "    # calculate deltas\n",
        "    wa = (x1-x) * (y1-y)\n",
        "    wb = (x1-x) * (y-y0)\n",
        "    wc = (x-x0) * (y1-y)\n",
        "    wd = (x-x0) * (y-y0)\n",
        "\n",
        "    # add dimension for addition\n",
        "    wa = tf.expand_dims(wa, axis=3)\n",
        "    wb = tf.expand_dims(wb, axis=3)\n",
        "    wc = tf.expand_dims(wc, axis=3)\n",
        "    wd = tf.expand_dims(wd, axis=3)\n",
        "\n",
        "    # compute output\n",
        "    out = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n",
        "\n",
        "    return out\n",
        "\n",
        "# identity transform\n",
        "theta = np.array([[1., 0, 0], [0, 1., 0]])"
      ],
      "metadata": {
        "id": "oNzMvmye00Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from keras_unet_collection.layer_utils import *\n",
        "from keras_unet_collection.activations import GELU, Snake\n",
        "from keras_unet_collection._model_unet_2d import UNET_left, UNET_right\n",
        "from keras_unet_collection.transformer_layers import patch_extract, patch_embedding\n",
        "from keras_unet_collection._backbone_zoo import backbone_zoo, bach_norm_checker\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, MultiHeadAttention, LayerNormalization, Dense, Embedding\n",
        "\n",
        "\n",
        "############################# CONVOLUTIONAL BLOCK #############################\n",
        "\n",
        "def CONV_stack(X, channel, kernel_size=3, stack_num=2,\n",
        "               dilation_rate=1, activation='ReLU',\n",
        "               batch_norm=False, name='conv_stack'):\n",
        "\n",
        "    bias_flag = not batch_norm\n",
        "\n",
        "    # stacking Convolutional layers\n",
        "    for i in range(stack_num):\n",
        "\n",
        "        activation_func = eval(activation)\n",
        "\n",
        "        # linear convolution\n",
        "        X = Conv2D(channel, kernel_size, padding='same', use_bias=bias_flag,\n",
        "                   dilation_rate=dilation_rate, name='{}_{}'.format(name, i))(X)\n",
        "\n",
        "\n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=3, name='{}_{}_bn'.format(name, i))(X)\n",
        "\n",
        "        # activation\n",
        "        activation_func = eval(activation)\n",
        "        X = activation_func(name='{}_{}_activation'.format(name, i))(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def UNET_att_right(X, X_left, channel, att_channel, kernel_size=3, stack_num=2,\n",
        "                   activation='ReLU', atten_activation='ReLU', attention='add',\n",
        "                   unpool=True, batch_norm=False, name='right0'):\n",
        "\n",
        "    pool_size = 2\n",
        "\n",
        "    X = decode_layer(X, channel, pool_size, unpool,\n",
        "                     activation=activation, batch_norm=batch_norm, name='{}_decode'.format(name))\n",
        "\n",
        "    X_left = attention_gate(X=X_left, g=X, channel=att_channel, activation=atten_activation,\n",
        "                            attention=attention, name='{}_att'.format(name))\n",
        "\n",
        "    # Tensor concatenation\n",
        "    H = concatenate([X, X_left], axis=-1, name='{}_concat'.format(name))\n",
        "\n",
        "    # stacked linear convolutional layers after concatenation\n",
        "    H = CONV_stack(H, channel, kernel_size, stack_num=stack_num, activation=activation,\n",
        "                   batch_norm=batch_norm, name='{}_conv_after_concat'.format(name))\n",
        "\n",
        "    return H\n",
        "\n",
        "\n",
        "\n",
        "def transunet_2d_base(input_tensor, filter_num, stack_num_down=2, stack_num_up=2,\n",
        "                      embed_dim=768, num_mlp=3072, num_heads=4, num_transformer=4,\n",
        "                      activation='ReLU', atten_activation='ReLU', attn_kernel_size=7,attn_drop_rate=0.4, attention='add', mlp_activation='GELU', batch_norm=False, pool=True, unpool=True,\n",
        "                      backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='transunet'):\n",
        "\n",
        "    activation_func = eval(activation)\n",
        "\n",
        "    X_skip = []\n",
        "    depth_ = len(filter_num)\n",
        "\n",
        "    # ----- internal parameters ----- #\n",
        "\n",
        "    # patch size (fixed to 1-by-1)\n",
        "    patch_size = 1\n",
        "\n",
        "    # input tensor size\n",
        "    input_size = input_tensor.shape[1]\n",
        "\n",
        "    # encoded feature map size\n",
        "    encode_size = input_size // 2**(depth_-1)\n",
        "\n",
        "    # number of size-1 patches\n",
        "    num_patches = encode_size ** 2\n",
        "\n",
        "    # dimension of the attention key (= dimension of embedings)\n",
        "    key_dim = embed_dim\n",
        "\n",
        "    # number of MLP nodes\n",
        "    filter_num_MLP = [num_mlp, embed_dim]\n",
        "\n",
        "    # ----- UNet-like downsampling ----- #\n",
        "\n",
        "    # no backbone cases\n",
        "    if backbone is None:\n",
        "\n",
        "        X = input_tensor\n",
        "\n",
        "        # stacked conv2d before downsampling\n",
        "\n",
        "        X = CONV_stack(X, filter_num[0], stack_num=stack_num_down, activation=activation,\n",
        "                       batch_norm=batch_norm, name='{}_down0'.format(name))\n",
        "\n",
        "        X_skip.append(X)\n",
        "\n",
        "        # downsampling blocks\n",
        "        for i, f in enumerate(filter_num[1:]):\n",
        "            X = UNET_left(X, f, stack_num=stack_num_down, activation=activation, pool=pool,\n",
        "                          batch_norm=batch_norm, name='{}_down{}'.format(name, i+1))\n",
        "            X_skip.append(X)\n",
        "\n",
        "    # backbone cases\n",
        "    else:\n",
        "        # handling VGG16 and VGG19 separately\n",
        "        if 'VGG' in backbone:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_skip = backbone_([input_tensor,])\n",
        "            depth_encode = len(X_skip)\n",
        "\n",
        "        # for other backbones\n",
        "        else:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_-1, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_skip = backbone_([input_tensor,])\n",
        "            depth_encode = len(X_skip) + 1\n",
        "\n",
        "\n",
        "        # extra conv2d blocks are applied\n",
        "        # if downsampling levels of a backbone < user-specified downsampling levels\n",
        "        if depth_encode < depth_:\n",
        "\n",
        "            # begins at the deepest available tensor\n",
        "            X = X_skip[-1]\n",
        "\n",
        "            # extra downsamplings\n",
        "            for i in range(depth_-depth_encode):\n",
        "                i_real = i + depth_encode\n",
        "\n",
        "                X = UNET_left(X, filter_num[i_real], stack_num=stack_num_down, activation=activation, pool=pool,\n",
        "                              batch_norm=batch_norm, name='{}_down{}'.format(name, i_real+1))\n",
        "                X_skip.append(X)\n",
        "\n",
        "    # subtrack the last tensor (will be replaced by the ViT output)\n",
        "    X = X_skip[-1]\n",
        "    X_skip = X_skip[:-1]\n",
        "\n",
        "    # 1-by-1 linear transformation before entering ViT blocks\n",
        "\n",
        "    X = Conv2D(filter_num[-1], 1, padding='valid', use_bias=False, name='{}_conv_trans_before'.format(name))(X)\n",
        "\n",
        "    # stacked ViTs\n",
        "    for i in range(num_transformer):\n",
        "\n",
        "        X = ViT_block(X, num_heads, key_dim, filter_num_MLP, activation=mlp_activation,\n",
        "                      name='{}_ViT_{}'.format(name, i))\n",
        "\n",
        "\n",
        "    # reshape patches to feature maps\n",
        "    X = tf.reshape(X, (-1, encode_size, encode_size, embed_dim))\n",
        "\n",
        "    # 1-by-1 linear transformation to adjust the number of channels\n",
        "\n",
        "    X = spatial_transformer_network(X, theta=theta)\n",
        "    X_skip.append(X)\n",
        "\n",
        "    # ----- UNet-like upsampling ----- #\n",
        "\n",
        "    # reverse indexing encoded feature maps\n",
        "    X_skip = X_skip[::-1]\n",
        "    # upsampling begins at the deepest available tensor\n",
        "    X = X_skip[0]\n",
        "    # other tensors are preserved for concatenation\n",
        "    X_decode = X_skip[1:]\n",
        "    depth_decode = len(X_decode)\n",
        "\n",
        "    # reverse indexing filter numbers\n",
        "    filter_num_decode = filter_num[:-1][::-1]\n",
        "\n",
        "    # upsampling with concatenation\n",
        "    for i in range(depth_decode):\n",
        "\n",
        "        f = filter_num_decode[i]\n",
        "\n",
        "        X = UNET_att_right(X, X_decode[i], f, att_channel=f//2, stack_num=stack_num_up,\n",
        "                           activation=activation, atten_activation=atten_activation, attention=attention,\n",
        "                           unpool=unpool, batch_norm=batch_norm, name='{}_up{}'.format(name, i))\n",
        "\n",
        "    return X\n",
        "\n",
        "def etransunet(input_size, filter_num, n_labels, stack_num_down=2, stack_num_up=2,\n",
        "                 embed_dim=128, num_mlp = 128, num_heads=4, num_transformer=4,\n",
        "                 activation='ReLU', atten_activation='ReLU',attn_kernel_size=7, attn_drop_rate=0.4, attention='add', mlp_activation='GELU', output_activation='Sigmoid', batch_norm=False, pool=True, unpool=True,\n",
        "                 backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='transunet'):\n",
        "\n",
        "    activation_func = eval(activation)\n",
        "\n",
        "    IN = Input(input_size)\n",
        "\n",
        "    # base\n",
        "    X = transunet_2d_base(IN, filter_num, stack_num_down=stack_num_down, stack_num_up=stack_num_up,\n",
        "                          embed_dim=embed_dim, num_mlp=num_mlp, num_heads=num_heads, num_transformer=num_transformer,\n",
        "                          activation=activation, attn_kernel_size=attn_kernel_size, attn_drop_rate=attn_drop_rate, atten_activation=atten_activation, attention=attention, mlp_activation=mlp_activation, batch_norm=batch_norm, pool=pool, unpool=unpool,\n",
        "                          backbone=backbone, weights=weights, freeze_backbone=freeze_backbone, freeze_batch_norm=freeze_batch_norm, name=name)\n",
        "\n",
        "    # output layer\n",
        "    OUT = CONV_output(X, n_labels, kernel_size=1, activation=output_activation, name='{}_output'.format(name))\n",
        "    #OUT = tf.reshape(OUT, [128,128,1])\n",
        "\n",
        "\n",
        "    # functional API model\n",
        "    model = Model(inputs=[IN,], outputs=[OUT,], name='{}_model'.format(name))\n",
        "    model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy', precision, recall, f1])\n",
        "\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Abj-YCFf0-WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=etransunet(input_size=(128, 128, 3), filter_num=[32,64,128], n_labels=1)\n"
      ],
      "metadata": {
        "id": "PtW9aeHo1TV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}